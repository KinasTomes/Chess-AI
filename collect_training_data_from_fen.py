import os
import chess
import chess.pgn
import chess.engine
import numpy as np
import ray
import torch

from typing import Dict
from training.timer import Timer
from core.chess_base import ChessEnv
from training.replay_buffer import ReplayBuffer
from core.coords_converter import ChessCoordsConverter
from utils.stockfish_worker import StockfishWorker, parallel_evaluate_legal_moves

stockfish_path = r'evaluation\stockfish\stockfish-windows-x86-64-avx2.exe'

def split_pgn_file(input_pgn_path: str, output_dir: str = None) -> None:
    """
    T√°ch m·ªôt file PGN l·ªõn th√†nh c√°c file PGN nh·ªè, m·ªói file ch·ª©a m·ªôt v√°n c·ªù.

    Args:
        input_pgn_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PGN g·ªëc.
        output_dir (str): Th∆∞ m·ª•c ƒë·ªÉ l∆∞u c√°c file PGN nh·ªè.
    """
    if not output_dir is None and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(input_pgn_path, encoding='utf-8') as pgn_file:
        game_idx = 1
        while game_idx < 2000:
            game = chess.pgn.read_game(pgn_file)
            if game is None:
                break

            if output_dir is None:
                output_path = f"game_{game_idx}.pgn"
            else:
                output_path = os.path.join(output_dir, f"game_{game_idx}.pgn")

            
            with open(output_path, 'w', encoding='utf-8') as output_file:
                exporter = chess.pgn.StringExporter(headers=True, variations=True, comments=True)
                output_file.write(game.accept(exporter))
            print(f"‚úÖ Saved game {game_idx} to {output_path}")
            game_idx += 1

    print(f"\nüéØ T·ªïng c·ªông {game_idx - 1} v√°n ƒë√£ ƒë∆∞·ª£c t√°ch v√† l∆∞u trong th∆∞ m·ª•c '{output_dir}'.")

def get_fen_and_moves_from_pgn_file(pgn_path: str) -> tuple[list[str], list[str]]:
    with open(pgn_path, 'r', encoding='utf-8') as pgn_file:
        game = chess.pgn.read_game(pgn_file)
        if game is None:
            print("‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c v√°n c·ªù.")
            return [], []

        board = game.board()
        fen_list = []
        uci_moves = []

        for move in game.mainline_moves():
            fen_list.append(board.fen())
            uci_moves.append(move.uci())
            board.push(move)

        return fen_list, uci_moves
    
def map_legal_move_scores_to_policy(legal_moves: Dict[str, float], action_dim: int = 4864) -> np.ndarray:
    policy = np.zeros(action_dim, dtype=np.float32)
    converter = ChessCoordsConverter()
    for move_uci, score in legal_moves.items():
        move_index = converter.move_to_index(chess.Move.from_uci(move_uci))
        policy[move_index] = score
    return policy
    
def get_replay_buffer_from_pgn(pgn_path: str) -> ReplayBuffer:
    # Kh·ªüi t·∫°o Ray
    ray.init(num_cpus=8)
    
    # Kh·ªüi t·∫°o c√°c worker
    num_workers = 8  # S·ªë worker b·∫±ng s·ªë CPU cores
    workers = [StockfishWorker.remote(stockfish_path) for _ in range(num_workers)]
    
    try:
        fens, moves = get_fen_and_moves_from_pgn_file(pgn_path)
        w, b = (1, 2) if chess.Board(fens[0]).turn else (2, 1)
        env = ChessEnv(white_player_id=w, black_player_id=b)
        replay_buffer = ReplayBuffer()
        game_history = []

        for move in moves:
            scores = parallel_evaluate_legal_moves(env.chess_board, workers)
            game_history.append({
                'state': env._observation(),
                'policy': map_legal_move_scores_to_policy(scores),
                'player': env.to_play
            })
            env.step(env.chess_coords.move_to_index(chess.Move.from_uci(move)))

        if env.winner == env.white_player:
            game_result = 1
        elif env.winner == env.black_player:
            game_result = -1    
        else:
            game_result = 0

        for history in game_history:
            if history['player'] == env.white_player:
                value = game_result
            else:
                value = -game_result

            replay_buffer.add_game([(history['state'], history['policy'], value)])
        
        return replay_buffer
        
    finally:
        # T·∫Øt c√°c worker
        for worker in workers:
            worker.shutdown.remote()
        # T·∫Øt Ray
        ray.shutdown()

def process_pgn_directory_to_buffer(directory_path: str, save_dir: str = "buffer_data", samples_per_file: int = 10000) -> None:
    """
    X·ª≠ l√Ω t·∫•t c·∫£ c√°c file PGN trong th∆∞ m·ª•c v√† l∆∞u th√†nh c√°c file buffer.

    Args:
        directory_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c file PGN
        save_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u c√°c file buffer
        samples_per_file: S·ªë samples t·ªëi ƒëa trong m·ªói file buffer
    """
    # T·∫°o th∆∞ m·ª•c l∆∞u n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(save_dir, exist_ok=True)
    
    # L·∫•y danh s√°ch c√°c file PGN
    pgn_files = [f for f in os.listdir(directory_path) if f.endswith('.pgn')]
    print(f"üéØ T√¨m th·∫•y {len(pgn_files)} file PGN trong th∆∞ m·ª•c {directory_path}")
    
    # Kh·ªüi t·∫°o Ray v√† c√°c worker
    ray.init(num_cpus=8)
    num_workers = 8
    workers = [StockfishWorker.remote(stockfish_path) for _ in range(num_workers)]
    
    try:
        # Buffer t·∫°m th·ªùi ƒë·ªÉ gom samples
        temp_buffer = ReplayBuffer()
        current_part = 0
        
        # X·ª≠ l√Ω t·ª´ng file PGN
        for pgn_file in pgn_files:
            print(f"\nüìÇ ƒêang x·ª≠ l√Ω file: {pgn_file}")
            pgn_path = os.path.join(directory_path, pgn_file)
            
            # ƒê·ªçc file PGN
            with open(pgn_path, 'r', encoding='utf-8') as pgn_file:
                game = chess.pgn.read_game(pgn_file)
                if game is None:
                    print(f"‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c v√°n c·ªù t·ª´ file {pgn_path}")
                    continue

                board = game.board()
                moves = list(game.mainline_moves())
                
                # X√°c ƒë·ªãnh ng∆∞·ªùi ch∆°i
                w, b = (1, 2) if board.turn else (2, 1)
                env = ChessEnv(white_player_id=w, black_player_id=b)
                game_history = []

                # X·ª≠ l√Ω t·ª´ng n∆∞·ªõc ƒëi
                for move in moves:
                    # ƒê√°nh gi√° c√°c n∆∞·ªõc ƒëi h·ª£p l·ªá
                    scores = parallel_evaluate_legal_moves(env.chess_board, workers)
                    
                    # L∆∞u tr·∫°ng th√°i v√† policy
                    game_history.append({
                        'state': env._observation(),
                        'policy': map_legal_move_scores_to_policy(scores),
                        'player': env.to_play
                    })
                    
                    # Th·ª±c hi·ªán n∆∞·ªõc ƒëi
                    env.step(env.chess_coords.move_to_index(move))

                # X√°c ƒë·ªãnh k·∫øt qu·∫£ v√°n ƒë·∫•u
                if env.winner == env.white_player:
                    game_result = 1
                elif env.winner == env.black_player:
                    game_result = -1    
                else:
                    game_result = 0

                # Th√™m v√†o buffer
                temp_game_data = []
                for history in game_history:
                    if history['player'] == env.white_player:
                        value = game_result
                    else:
                        value = -game_result

                    temp_game_data.append((history['state'], history['policy'], value))
                    
                temp_buffer.add_game(temp_game_data)
                # Ki·ªÉm tra xem c√≥ ƒë·ªß samples ƒë·ªÉ l∆∞u file m·ªõi kh√¥ng
                if len(temp_buffer) >= samples_per_file:
                    # L·∫•y samples_per_file samples ƒë·∫ßu ti√™n
                    states, policies, values = temp_buffer.sample_batch(samples_per_file)
                    
                    # L∆∞u th√†nh file ri√™ng
                    save_path = os.path.join(save_dir, f"buffer_part_{current_part}.pt")
                    torch.save({
                        'states': states,
                        'policies': policies,
                        'values': values
                    }, save_path)
                    print(f"üíæ ƒê√£ l∆∞u file {save_path} v·ªõi {len(states)} samples")
                    
                    # X√≥a samples ƒë√£ l∆∞u kh·ªèi buffer
                    temp_buffer.clear(10000)
                    current_part += 1
        
        # L∆∞u ph·∫ßn samples c√≤n l·∫°i n·∫øu c√≥
        if len(temp_buffer) > 0:
            states, policies, values = temp_buffer.sample_batch(len(temp_buffer))
            save_path = os.path.join(save_dir, f"buffer_part_{current_part}.pt")
            torch.save({
                'states': states,
                'policies': policies,
                'values': values
            }, save_path)
            print(f"üíæ ƒê√£ l∆∞u file {save_path} v·ªõi {len(states)} samples")
            
    finally:
        # T·∫Øt c√°c worker
        for worker in workers:
            worker.shutdown.remote()
        # T·∫Øt Ray
        ray.shutdown()

if __name__ == "__main__":
    timer = Timer()
    timer.start()
    
    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c file PGN trong th∆∞ m·ª•c
    process_pgn_directory_to_buffer(
        directory_path="sample_data/fen_database",
        save_dir="buffer_data",
        samples_per_file=10000
    )
    
    timer.end()